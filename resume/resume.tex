\documentclass[]{elsarticle}

\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[margin=0.8in]{geometry}
\usepackage{color}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\hypersetup{colorlinks=true, citecolor=blue}

\usepackage{tocloft}

%opening

\begin{document}
	
	\onecolumn
	
	\begin{abstract}
		In precision agriculture it's common to use multi-spectral camera (RGB+NIR).
		Today the necessarity of having hight spatial resolution and numerous spectral band (6,8,+), have bring new type of cameras such as multi-sensor one,
		in order to compute spectral index and extract informations such as shape, texture, \dots for plants and leaves analysis.
		The problem with these cameras is that all the sensors are not aligned, so we need a solution to align the different images with precision in close view.
		
		In this study we propose a two step method applied to Airphen camera (6 spectral images).
		(i) Affine correction using pre-calibrated matrix at different height, the closest transformation can be selected via internal GPS.
		And (ii) Perspective correction to refine the previous one, using keypoints matching between enhanced gradients of each spectral bands.
		The contributions of this paper are the evaluation of different types of keypoints detection and their benchmark.
	\end{abstract}
	
	\begin{keyword}
		Registration \sep
		Multi-spectral imagery \sep
		Precision farming \sep
		Feature descriptor
	\end{keyword}
	
	\begin{frontmatter}
		\title{Two steep multi-spectral registration \\ via keypoint detector and gradient similarity}
		\author[unilu]{Jehan-Antoine VAYSSADE} \ead{jehan-antoine.vayssade@inra.fr}
		\address[myuni]{Agrosup D2A2E pole GestAd equipe agriculture de precision 21000 Dijon, France}
		\date{Received: date / Accepted: date}
	\end{frontmatter}
	
	\newpage
	\twocolumn
	
	\section{Introduction}
	
	Image registration is the process of transforming different images of one scene into the same coordinate system.
	The spatial relationships between these images can be rigid (translations and rotations), affine (shears for example),
	homography, or complex large deformation models (due to the difference of depth between ground and leafs) \cite{Kamoun}.
	The main difficulty is that multi-spectral images have wavelength with high distance between each spectral bands.
	Which implies (i) leaves have a different aspect depending on the spectral bands
	(ii) there are highly complex and self-similar structures in our images
	(iii) the scene are a grassland or agriculture image at different scale, which is a complex spectral scene making a hard fit for such a registration.
	\\
	\par There is two types of registration, feature based and intensity based \cite{Zitova}.
	(i) Feature based methods use feature matching, in most cases a bruteforce matching is used, making those techniques slow.
	Fortunately these features can be filtered to reduce the matching cost depending of the spatial properties we have, and a GPGPU implementation can reduce the comparisons' cost.
	(ii) Intensity-based automatic image registration is an iterative process, and the metrics used are sensitive to determine the numbers of iteration,
	making such method even worth in time for precise registration. Furthermore in multi-spectral we need different metrics for each registered bands which is hard to achieve.
	\\
	\par Different studies of images alignment using multi-sensors camera exist using UAV.
	Some show good performances for feature based \cite{DantasDiasJunior, Vakalopoulou} with strong enhancement of feature descriptor for matching performances.
	Other don't and prefer to use intensity based \cite{douarre:hal-02183837} with better convergence metrics, which is slower and not necessarily robust against light variabilities.
	\\
	\par Unless this type of articles, as we know, no studies have been made under agricultural and external conditions in near field of view (less than 10 meter).
	Those studies mainly propose features matching without large methods comparison of their performance (time/precision),
	spectral band reference selection, or pre-affine correction depending on the distance.
	Thus, this study propose the best combination of feature extractor and spectral reference on normalized gradients transformation,
	using pre-affine registration and matches filtering, evaluated at different spatial resolution.
	%\\
	%\par In this study we have preferred not to enhance features by the information send to each features methods,
	%as example SIFT have been rejected on the paper \cite{douarre:hal-02183837}
	%which explain that the matched features are two numerous and not greatly matched.
	
	\subsection{Material}
	
	The multi-spectral imagery was provided by the six-band multi-spectral camera Airphen \footnote{\url{https://www.hiphen-plant.com/our-solutions/airphen/}}.
	AIRPHEN is a scientific multi spectral camera developed by agronomists for agricultural applications.
	It can be embedded in different types of platforms such as UAV, phenotyping robots, etc.
	AIRPHEN is highly configurable (bands, fields of view), lightweight and compact.
	It can be operated wireless and combined with complementary thermal infrared channel and high resolution RGB cameras.
	The camera was configured using 450/570/675/710/730/850 nm with FWHM of 10nm.
	The focal lens is 8mm. It's raw resolution for each spectral band is 1280x960 px with 12 bit of precision.
	Finally the camera also provide an internal GPS antenna, that can be used to get the distance from the ground.
	
	%\begin{figure}[!htb]
	%\centering
	%\includegraphics[height=15em]{../figures/airphen-detail3.png}
	%\caption{the airphen camera}
	%\label{fig:airphen}
	%\end{figure}
	
	\subsection{Data}
	
	Two datasets were taken at different heights.
	We have used a ``stairville LB-3s lighting stand set'' like for positioning the camera at different heights.
	Due to the size of the chessboard, the limited focus of the camera and the height of the lighting stand set,
	we have bounded the acquisition height from 1.6 meter to 5 meter with 20cm steep.
	
	The first dataset is for the calibration. A chessboard is taken at different heights, the corresponding data can be found in data/steep-chess/.
	And the figure \ref{fig:calibration} shows the chessboard taken at each distance from the ground.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/calibration-height.jpg}
		\caption{chessboard for calibration at different height}
		\label{fig:calibration}
	\end{figure}
	
	The second dataset is for the alignment verification. One shot of a grassland is taken at different heights, the corresponding data can be found in data/steep/
	with a bias of +/- 10cm to be in the worst case (most complex).
	
	\newpage
	\section{Method}
	
	Alignment is refined in two stages, with
	(i) affine registration roughly estimated
	and (ii) perspective registration for the refinement and precision.
	As example the figure \ref{fig:merged-correction} show each correction steep at 1.6 meters.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/merged-correction.png}
		\caption{Exemple of each correction}
		\label{fig:merged-correction}
	\end{figure}
	
	\subsection{Affine Correction}
	
	It's important to notice the closer we take the snapshot, the bigger is the distance of the initial Affine Correction.
	On the other hand at a distance superior or equals to 5 meters, the initial affine correction become stable (figure \ref{fig:affine-translation-height}).
	A calibration is used to build a linear model, which makes the affine correction to work at any height.
	The main purpose of this step is to reduce the distance of each spectral band,
	which allow the similarity to be spatially delimited within a few pixels, making feature matching more effective.
	
	\paragraph{Detect chessboard}
	We use opencv 4 \textit{findChessboardCorners} for each spectral image (normalized) at different heights (from 1.6 to 5 meters).
	The function attempts to determine whether the input image is a view of the chessboard pattern and locate the internal chessboard corners.
	The detected coordinates are approximated, and to determine their positions more accurately we use the function \textit{cornerSubPix} as explained in the documentation.
	The detected points are ordered by x/y (detection can be flipped) and saved on data/'height'.npy
	
	\paragraph{Making linear model}
	
	Using all the points detected for each spectral band, we calculate the centroid grid (each point average).
	The affine transform from each spectral band to this centroid grid is estimated.
	It appear that the rotation and the scale do not depend on the distance to the ground, but only on the translation.
	This is expected, so that only one calibration can be used for this part of the matrix.
	This is visible on the figure \ref{fig:affine-translation-height}.
	The factor $a,b,c,d$ is the rotation and scale factor that is quite stable and close to identity (accuracy depends on the spatial resolution of the board).
	On the other hand, the translation in $x, y$ depend on the height.
	
	Thus a Levenberg-Marquardt curve fitting algorithm with linear least squares regression \cite{More78}
	can be used to fit an equation for each spectral band again $x$ and $y$ independently to the centroid grid.
	We have chosen to adjust the following curve $y = ax^3 + bx^2 + cx + d$ where $x$ is the height,
	$y$ is the resulted translation and factors $a,b,c,d$ are the model parameter.
	%The fitted parameters for each spectral bands can be found in supplementary data.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/affine-translation-height.png}
		\caption{Affine matrix value by height}
		\label{fig:affine-translation-height}
	\end{figure}
	
	\paragraph{Correction}
	
	To make the affine matrix correction, we used the factors $a,b,c,d$ at the most accurate height (1.6 meter).
	For the translation part, the curve model is applied for each spectral bands at the given height (roughly known by the user or using the internal GPS sensor).
	Each spectral band are so warped using the corresponding affine transformation.
	Finally, all spectral bands are cropped to the minimal bounding box (minimal and maximal translation of each affine matrix).
	
	\subsection{Perspective correction}
	%Once the best keypoints extractor and spectral reference are defined, we use there detection to estimate an homography.
	%Homography is an isomorphism of perspectives. A 2D homography between A and B would give you the projection transformation
	%between the two images. It is a 3x3 matrix that describes the affine transformation.
	
	
	Each spectral band has different properties and values by nature. (figure \ref{fig:vegetable-gradient})
	But we can extract the corresponding similarity by transforming each spectral band into its absolute derivative,
	to find similarity in gradient break among them.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/contrast-inversion.png}
		\caption{Gradient orientation in spectral band \cite{rabatel:hal-01684135}}
		\label{fig:vegetable-gradient}
	\end{figure}
	
	\par The previous correction, such as Affine correction attempts to help the feature matching by adding properties of epipolar lines (close).
	Thus, the correspondence of the extracted features can be spatially bounded,
	(i) we know that the maximum translation is limited to a distance of a few pixels (less than $10$px),
	and (ii) the angle between the initial element and the matched one is limited to $[-1,1]$ degree.
	
	\paragraph{Computing the gradient} To compute the gradient of the image with a minimal impact of the light distribution (shadow, reflectance, specular, ...)
	Each spectral band are normalized using Gaussian blur \cite{sage0303}, the kernel size is defined by $math.ceil(image\_width^{0.4}) // 2 * 2 +1$ (19 in your case)
	and the final normalized image are defined by $i/(G+1)*255$ where $i$ is the spectral band and $G$ are the Gaussian blur of those spectral band.
	This first steep allow to minimize the impact of the noise on the gradient and smoothes the signal in case of hight reflectance.
	Using this normalized image, the gradient are computed with the sum of Sharr filter \cite{Seitz} again $d_x=1$ and $d_y=1$.
	
	Different types of edge detection such as Sobel, Laplacan and Canny were tested unsuccessfully (without sufficient number of matches), those not included in this study. Finally, all gradients are normalized using CLAHE \cite{zuiderveld1994contrast} to locally improve their intensity and increase the number of key points detected (especially for 850nm).
	%\begin{figure}[!htb]
	%	\centering
	%	\includegraphics[width=0.6\linewidth]{../figures/math-perspective-correction.png}
	%	\caption{equation of the perspective correction}
	%	\label{eq:perspective}
	%\end{figure}
	
	\paragraph{Keypoints Extractor}
	A key point is a point of interest. It defines what is important and distinctive in an image.
	Different types of key point extractors were tested, all the results can be found in "figures/*".
	These algorithms are all available and easily usable in OpenCV.
	%For all these algorithms, we use the default settings.
	%In some cases, the parameters are set to increase or decrease the number of key points
	%(trying to have a minimum of 20 corresponding key points, and less than 500)..
	
	\begin{itemize}
		\item ORB : An efficient alternative to SIFT or SURF
		\subitem[1] nfeatures=5000
		\subitem[2] nfeatures=10000
		\subitem[3] nfeatures=15000
		
		\item AKAZE : Fast explicit diffusion for accelerated features in nonlinear scale spaces
		\subitem[1] nOctaves=1, nOctaveLayers=1
		\subitem[2] nOctaves=2, nOctaveLayers=1
		\subitem[3] nOctaves=2, nOctaveLayers=2
		
		\item KAZE : A novel multi-scale 2D feature detection and description algorithm in nonlinear scale spaces \cite{rs10050756}
		\subitem[1] nOctaves=4, nOctaveLayers=2
		\subitem[2] nOctaves=4, nOctaveLayers=4
		\subitem[3] nOctaves=2, nOctaveLayers=4
		
		\item BRISK : Binary robust invariant scalable keypoints.
		\subitem[1] nOctaves=0, patternScale=.1
		\subitem[2] nOctaves=1, patternScale=.1
		\subitem[3] nOctaves=2, patternScale=.1
		
		\item AGAST : Adaptive and generic corner detection based on the accelerated segment test
		\subitem[1] threshold=71, nonmaxSuppression=True
		\subitem[2] threshold=92, nonmaxSuppression=True
		\subitem[3] threshold=163, nonmaxSuppression=True
		
		\item MSER : maximally stable extremal regions
		%\item SIFT : \cite{AguileraCarrasco2012MultispectralIF}
		
		\item SURF : Speed-Up Robust Features
		\subitem[1] nOctaves=1, nOctaveLayers=1
		\subitem[2] nOctaves=2, nOctaveLayers=1
		\subitem[3] nOctaves=2, nOctaveLayers=2
		
		\item FAST : FAST Algorithm for Corner Detection
		\subitem[1] threshold=71, nonmaxSuppression=True
		\subitem[2] threshold=92, nonmaxSuppression=True
		\subitem[3] threshold=163, nonmaxSuppression=True
		
		\item GFTT : Good Features to Track
		\subitem[1] maxCorners=5000,useHarrisDetector=True
		\subitem[2] maxCorners=10000,useHarrisDetector=True
		\subitem[3] maxCorners=15000,useHarrisDetector=True
	\end{itemize}
	
	\paragraph{Keypoint detection}
	We use one of the keypoint extractors mentioned above between each spectral band gradient (all extractors are evaluated).
	For each detected keypoint, we extract a descriptor using ORB features.
	We match all detected keypoints to a reference spectral band (all bands are evaluated).
	All matches are filtered (distance, position, angle) to eliminate false positives along the epipolar line.
	Finally we use the function \textit{findHomography} between the key points detected/filtered with RANSAC,
	to determine the best subset of matches to calculate the perspective correction.
	
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=0.7\linewidth]{../figures/prespective-feature-matching.jpg}
		\caption{feature matching}
		\label{fig:feature-matching}
	\end{figure}
	
	\paragraph{Correction}
	
	The perspective correction between each spectral band to the reference is estimated and applied.
	Finally, all spectral bands are cropped to the minimum bbox,
	the minimum and maximum points are obtained by applying a perspective transformation to each corner of the image.
	
	%\subsection{Re-estimating the reel height}
	%The same procedure about curve fitting \cite{More78} can be used to evaluate the inverse model between height and translation.
	%By adding the affine translation and the perspective transform of the central image point $x,y$,
	%the ``real'' translation can be evaluated. Using this value as input of the inverse model,
	%we can estimate the real height of the acquisition.
	%
	%\noindent
	%\colorbox{green}{estimating the height of each spectral bands corner} \\
	%\colorbox{green}{to the reference can enable to build the ground plan ?} \\
	%\colorbox{green}{and enable to correct the row gradient ?}
	
	\section{Result and discussion}
	
	\paragraph{Affine correction}
	After the first correction, i.e. the Affine transformation using the linear model from the approximate height,
	the remaining distance between each spectral band varies according to the distance between the real height and the nearest selected.
	These residual distances can be seen in the figure \ref{fig:affine-error}.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/affine-allignement-rmse.jpg}
		\caption{The mean distance of detected keypoint before perspective correction}
		\label{fig:affine-error}
	\end{figure}
	
	It appear that the resulted rotation and scale do not depend depend on the ground distance, but only on the translation.
	Which expected, so single calibration can be used for this part of the matrix.
	For the translation part, it depends on the distance to the field, and can be estimated using the fft correlation \cite{506761}.
	Due to the hard correlation between spectral bands these registration, especially between 450nm and 710-850nm (unless using normalized gradient)
	have not been investigated and suggest that the reader should see the specific article \cite{rabatel:hal-01684135}.
	
	% https://www.lfd.uci.edu/~gohlke/code/imreg.py.html
	
	\paragraph{Keypoint matching} 
	The following figure \ref{fig:features-performances} shows the numbers of keypoints after filtering and homography association (minimum of all matches),
	the computation time and performance ratio (matches/time) for each method.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/comparaison-keypoint-performances.png}
		\caption{features performances}
		\label{fig:features-performances}
	\end{figure}
	
	\noindent
	All these methods work, the choice of method depends on how we want to balance between computation time and accuracy:
	\begin{itemize}
		\item GFTT show the best performance over all others both in computation time and number of matches
		\item FAST and AGAST is the most suitable, balanced between time and matches performances.
		\item KAZE show the best number of matches (>200) but it's also 2.5 times slower than FAST/AGAST.
	\end{itemize}
	
	\noindent
	The other ones did not show improvement in term of performances or matches:
	\begin{itemize}
		\item SURF the number of detected feature may not be enough to fit the perspective correction.
		\item AKAZE and MSER did not show benefits comparing to FAST.
		\item ORB could be excluded, the number of matches is near to ~20 how is the minimal to ensure that the homography is correct.
		\item BRISK show good number of matches, but there computation time is too huge (~79 sec) comparing to FAST (~8 sec).
	\end{itemize}
	
	Increasing the number of key points matched allows a slightly higher accuracy. For example, switch from SURF (~30 results) to FAST (~130 results). show the final residual distances reduced from ~1.2px to ~0.9px but the calculation time from ~5sec to ~8sec.

	All methods show that the best reference spectrum is 710nm, with the exception of SURF and GFTT which is 570nm.
	The following figure \ref{fig:features-FAST-performances} shows the minimum number of matches between each reference spectrum and all others using the FAST algorithm.
	Choosing the right spectral reference is important, as we can see, no correspondence is found in some cases between 650nm-850nm,
	but correspondences are found between 675nm-710nm and 710nm-850nm, making the 710nm more appropriate,
	the same behaviour can be observed for the other bands and 570nm.
	This is visible on the "all\_min" line, 570nm and 710nm have the best minimum number of matches.
	Other best spectral reference are available in supplementary material.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/comparaison-keypoint-matching-reference-FAST1.png}
		\caption{feature FAST performances}
		\label{fig:features-FAST-performances}
	\end{figure}

	\newpage
	
	\paragraph{Perspective correction}
	Residues of the perspective correction show that we have correctly registered each spectral band with a residual error of less than 1 pixel,
	the figure \ref{fig:perspective-error} shows the residual distance at different ground distances.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/prespective-allignement-rmse.jpg}
		\caption{Perspective Re-projection Error}
		\label{fig:perspective-error}
	\end{figure}
	
	The following figure \ref{fig:perspective-features-matching-scatter} shows the difference between detected points for two bands (red-green)
	before (left) and after (right) the perspective correction, and show that the residual errors are spatially uniform.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/perspective-features-matching-scatter.png}
		\caption{perspective-features-matching-scatter}
		\label{fig:perspective-features-matching-scatter}
	\end{figure}
	
	
	The decomposition of the residual distance by angle $[0-45-90-135-135-135-135-180-280-225-225-225-270-315-0]$ visible in the figure \ref{fig:residual-angle} is interesting.
	We can notice that the spatial distribution of the residues, for each different angle, is equally distributed.
	Our hypothesis is that the nature of the base information (spectral band + different lens) makes a small difference to the gradient break,
	which is detected by the features detector and propagated until the final correction (observed residue).
	This is interesting because these residues uniformly distributed by angle in space tend to minimize the resulting correction of its center (gradient),
	thus the detected residual error are overrated and should be less than $0.4$ pixel.
	\\
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{../figures/perspective-features-residual.png}
		\caption{Residual Distribution Again Angle}
		\label{fig:residual-angle}
	\end{figure}
	
	\par However, more complex deformation models \cite{Lombaert, ThinPlateSpline} can be used to improve the remaining error.
	This type of complex deformation has not been fully evaluated, but only quickly tested \textit{cv2.ThinPlateSplineShapeTransformer}.
	There does not seem to be any significant improvement in most cases (with a huge computation time).
	But can also, in some cases, create large angular deformations caused by the proximity of keypoints,
	of course, it is possible to filter these keypoints, which also reduces the overall accuracy.
	
	%%%%%%%%%%%%%%%%%%%
	
	\section{Conclusion}
	
	In this work, the application of different techniques for multi-spectral image registration was explored.
	We tested different methods of keypoints extractors at different heights and the number of control points obtained.
	As seen in the method, the most suitable method is the GFTT with a significant number of matches and a reasonable calculation time.
	
	Furthermore, the best spectral reference was defined for each method, for example 570 for GFTT.
	According to the figure \ref{fig:perspective-error} we observe a residual error of less than 1 px,
	supposedly caused by the difference in input (spectral range, lens).
	Finally, the method was tested on more than 8000 images in real conditions (not present in the study),
	randomly taken between 1.6 and 2.2 meters without registration error (always a minimum number of matches, without visible error, less than $0.9$px).
	\\
	\par Further researches can be performed on each parameter of the feature extractors, for those who need specific performance (time/precision).
	Otherwise feature matching can be optimized, at this stage, we use brute-force matching with post filtering,
	but a different implementation that fulfill our epipolar line properties should greatly enhance the number of matches by reducing false positives.
	
	\newpage
	
	\section{Acknowledgment}
	
	We would like to thanks Jones Gawain, Combaluzier Quentin, Michin Nicolas and Savi Romain
	for the realization of the ``lighting stand set'' that help us for positioning the camera at different heights.
	
	\section{Supplementary material}
	
	The additional data and source code associated with this article can be found in the online version at the following address
	\url{gitlab.com/phd-thesis-adventice/phd-airphen-alignment} the access is limited,
	and we invite you to send an email to the author for full access.
	
	\section{Reference}
	\bibliography{references.bib}
	\bibliographystyle{apalike}
	
\end{document}
